{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "187758b1",
   "metadata": {},
   "source": [
    "# Loading and Previewing the Dataset\n",
    "\n",
    "This is the first step in our course recommender machine learning project. Here we use the `pandas` library, a powerful tool for data manipulation and analysis in Python, to load our dataset and prepare it for further analysis. We start by reading a CSV file into a DataFrame, make a copy of the data, and then preview the first few rows to get an initial understanding of its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "832287d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>first_name</th>\n",
       "      <th>last_name</th>\n",
       "      <th>email</th>\n",
       "      <th>gender</th>\n",
       "      <th>extracurricular_activities</th>\n",
       "      <th>riasec</th>\n",
       "      <th>career_aspiration</th>\n",
       "      <th>math_score</th>\n",
       "      <th>history_score</th>\n",
       "      <th>physics_score</th>\n",
       "      <th>chemistry_score</th>\n",
       "      <th>biology_score</th>\n",
       "      <th>english_score</th>\n",
       "      <th>geography_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Paul</td>\n",
       "      <td>Casey</td>\n",
       "      <td>paul.casey.1@gslingacademy.com</td>\n",
       "      <td>male</td>\n",
       "      <td>False</td>\n",
       "      <td>27</td>\n",
       "      <td>Bachelor of Statistics</td>\n",
       "      <td>73</td>\n",
       "      <td>81</td>\n",
       "      <td>93</td>\n",
       "      <td>97</td>\n",
       "      <td>63</td>\n",
       "      <td>80</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Danielle</td>\n",
       "      <td>Sandoval</td>\n",
       "      <td>danielle.sandoval.2@gslingacademy.com</td>\n",
       "      <td>female</td>\n",
       "      <td>False</td>\n",
       "      <td>40</td>\n",
       "      <td>Bachelor of Supply Chain Management</td>\n",
       "      <td>90</td>\n",
       "      <td>86</td>\n",
       "      <td>96</td>\n",
       "      <td>100</td>\n",
       "      <td>90</td>\n",
       "      <td>88</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Tina</td>\n",
       "      <td>Andrews</td>\n",
       "      <td>tina.andrews.3@gslingacademy.com</td>\n",
       "      <td>female</td>\n",
       "      <td>True</td>\n",
       "      <td>30</td>\n",
       "      <td>Bachelor of Corporate Communications</td>\n",
       "      <td>81</td>\n",
       "      <td>97</td>\n",
       "      <td>95</td>\n",
       "      <td>96</td>\n",
       "      <td>65</td>\n",
       "      <td>77</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Tara</td>\n",
       "      <td>Clark</td>\n",
       "      <td>tara.clark.4@gslingacademy.com</td>\n",
       "      <td>female</td>\n",
       "      <td>False</td>\n",
       "      <td>40</td>\n",
       "      <td>Bachelor of Human Resouce Management</td>\n",
       "      <td>71</td>\n",
       "      <td>74</td>\n",
       "      <td>88</td>\n",
       "      <td>80</td>\n",
       "      <td>89</td>\n",
       "      <td>63</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Anthony</td>\n",
       "      <td>Campos</td>\n",
       "      <td>anthony.campos.5@gslingacademy.com</td>\n",
       "      <td>male</td>\n",
       "      <td>False</td>\n",
       "      <td>25</td>\n",
       "      <td>Bachelor of Development Studies</td>\n",
       "      <td>84</td>\n",
       "      <td>77</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>80</td>\n",
       "      <td>74</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id first_name last_name                                  email  gender  \\\n",
       "0   1       Paul     Casey         paul.casey.1@gslingacademy.com    male   \n",
       "1   2   Danielle  Sandoval  danielle.sandoval.2@gslingacademy.com  female   \n",
       "2   3       Tina   Andrews       tina.andrews.3@gslingacademy.com  female   \n",
       "3   4       Tara     Clark         tara.clark.4@gslingacademy.com  female   \n",
       "4   5    Anthony    Campos     anthony.campos.5@gslingacademy.com    male   \n",
       "\n",
       "   extracurricular_activities  riasec                     career_aspiration  \\\n",
       "0                       False      27                Bachelor of Statistics   \n",
       "1                       False      40   Bachelor of Supply Chain Management   \n",
       "2                        True      30  Bachelor of Corporate Communications   \n",
       "3                       False      40  Bachelor of Human Resouce Management   \n",
       "4                       False      25       Bachelor of Development Studies   \n",
       "\n",
       "   math_score  history_score  physics_score  chemistry_score  biology_score  \\\n",
       "0          73             81             93               97             63   \n",
       "1          90             86             96              100             90   \n",
       "2          81             97             95               96             65   \n",
       "3          71             74             88               80             89   \n",
       "4          84             77             65               65             80   \n",
       "\n",
       "   english_score  geography_score  \n",
       "0             80               87  \n",
       "1             88               90  \n",
       "2             77               94  \n",
       "3             63               86  \n",
       "4             74               76  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd  # Import the pandas library for data manipulation\n",
    "\n",
    "# Load the dataset from a CSV file into a DataFrame\n",
    "df1 = pd.read_csv(\"student-scores.csv\")\n",
    "\n",
    "# Create a copy of the DataFrame to preserve the original data\n",
    "df = df1.copy()\n",
    "\n",
    "# Display the first 5 rows of the DataFrame to understand the data structure\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5f3efa",
   "metadata": {},
   "source": [
    "# Understanding and Cleaning the Dataset\n",
    "\n",
    "We inspect the column names to understand the dataset's structure, then clean the data by removing irrelevant columns. This is essential to enable us focus on the features that contribute directly to our model's accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8e5e304f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['id', 'first_name', 'last_name', 'email'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m df\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Drop unnecessary columns that do not contribute to the course recommendation process\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfirst_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlast_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43memail\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Zeetracker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:5347\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   5199\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(\n\u001b[0;32m   5200\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   5201\u001b[0m     labels: IndexLabel \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5208\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   5209\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5210\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   5211\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[0;32m   5212\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5345\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[0;32m   5346\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 5347\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   5348\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5349\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5350\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5351\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5352\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5353\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5354\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5355\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Zeetracker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\generic.py:4711\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4709\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   4710\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 4711\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4713\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[0;32m   4714\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[1;32mc:\\Users\\Zeetracker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\generic.py:4753\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[1;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[0;32m   4751\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4752\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4753\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4754\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[0;32m   4756\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[0;32m   4757\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Zeetracker\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6992\u001b[0m, in \u001b[0;36mIndex.drop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   6990\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m   6991\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 6992\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask]\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6993\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[0;32m   6994\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['id', 'first_name', 'last_name', 'email'] not found in axis\""
     ]
    }
   ],
   "source": [
    "# Display the column names of the DataFrame to understand its structure\n",
    "df.columns\n",
    "\n",
    "# Drop unnecessary columns that do not contribute to the course recommendation process\n",
    "df.drop(columns=[\"id\", \"first_name\", \"last_name\", \"email\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ad3f4c",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "## Calculating Total and Average Scores\n",
    "\n",
    "This step in our course recommender project focuses on feature engineering, where we create new features from the existing data. Here, we calculate the total and average scores for each student across various subjects. These new features will be crucial in determining the student's overall performance, which can then be used to recommend appropriate courses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39b35efe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>extracurricular_activities</th>\n",
       "      <th>riasec</th>\n",
       "      <th>career_aspiration</th>\n",
       "      <th>math_score</th>\n",
       "      <th>history_score</th>\n",
       "      <th>physics_score</th>\n",
       "      <th>chemistry_score</th>\n",
       "      <th>biology_score</th>\n",
       "      <th>english_score</th>\n",
       "      <th>geography_score</th>\n",
       "      <th>total_score</th>\n",
       "      <th>average_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>male</td>\n",
       "      <td>False</td>\n",
       "      <td>27</td>\n",
       "      <td>Bachelor of Statistics</td>\n",
       "      <td>73</td>\n",
       "      <td>81</td>\n",
       "      <td>93</td>\n",
       "      <td>97</td>\n",
       "      <td>63</td>\n",
       "      <td>80</td>\n",
       "      <td>87</td>\n",
       "      <td>574</td>\n",
       "      <td>82.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>female</td>\n",
       "      <td>False</td>\n",
       "      <td>40</td>\n",
       "      <td>Bachelor of Supply Chain Management</td>\n",
       "      <td>90</td>\n",
       "      <td>86</td>\n",
       "      <td>96</td>\n",
       "      <td>100</td>\n",
       "      <td>90</td>\n",
       "      <td>88</td>\n",
       "      <td>90</td>\n",
       "      <td>640</td>\n",
       "      <td>91.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>female</td>\n",
       "      <td>True</td>\n",
       "      <td>30</td>\n",
       "      <td>Bachelor of Corporate Communications</td>\n",
       "      <td>81</td>\n",
       "      <td>97</td>\n",
       "      <td>95</td>\n",
       "      <td>96</td>\n",
       "      <td>65</td>\n",
       "      <td>77</td>\n",
       "      <td>94</td>\n",
       "      <td>605</td>\n",
       "      <td>86.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>female</td>\n",
       "      <td>False</td>\n",
       "      <td>40</td>\n",
       "      <td>Bachelor of Human Resouce Management</td>\n",
       "      <td>71</td>\n",
       "      <td>74</td>\n",
       "      <td>88</td>\n",
       "      <td>80</td>\n",
       "      <td>89</td>\n",
       "      <td>63</td>\n",
       "      <td>86</td>\n",
       "      <td>551</td>\n",
       "      <td>78.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>male</td>\n",
       "      <td>False</td>\n",
       "      <td>25</td>\n",
       "      <td>Bachelor of Development Studies</td>\n",
       "      <td>84</td>\n",
       "      <td>77</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>80</td>\n",
       "      <td>74</td>\n",
       "      <td>76</td>\n",
       "      <td>521</td>\n",
       "      <td>74.428571</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gender  extracurricular_activities  riasec  \\\n",
       "0    male                       False      27   \n",
       "1  female                       False      40   \n",
       "2  female                        True      30   \n",
       "3  female                       False      40   \n",
       "4    male                       False      25   \n",
       "\n",
       "                      career_aspiration  math_score  history_score  \\\n",
       "0                Bachelor of Statistics          73             81   \n",
       "1   Bachelor of Supply Chain Management          90             86   \n",
       "2  Bachelor of Corporate Communications          81             97   \n",
       "3  Bachelor of Human Resouce Management          71             74   \n",
       "4       Bachelor of Development Studies          84             77   \n",
       "\n",
       "   physics_score  chemistry_score  biology_score  english_score  \\\n",
       "0             93               97             63             80   \n",
       "1             96              100             90             88   \n",
       "2             95               96             65             77   \n",
       "3             88               80             89             63   \n",
       "4             65               65             80             74   \n",
       "\n",
       "   geography_score  total_score  average_score  \n",
       "0               87          574      82.000000  \n",
       "1               90          640      91.428571  \n",
       "2               94          605      86.428571  \n",
       "3               86          551      78.714286  \n",
       "4               76          521      74.428571  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the total score by summing the scores from all subjects\n",
    "df[\"total_score\"] = (\n",
    "    df[\"math_score\"]\n",
    "    + df[\"history_score\"]\n",
    "    + df[\"physics_score\"]\n",
    "    + df[\"chemistry_score\"]\n",
    "    + df[\"biology_score\"]\n",
    "    + df[\"english_score\"]\n",
    "    + df[\"geography_score\"]\n",
    ")\n",
    "\n",
    "# Calculate the average score by dividing the total score by the number of subjects\n",
    "df[\"average_score\"] = df[\"total_score\"] / 7\n",
    "\n",
    "# Display the first 5 rows of the DataFrame to verify the new columns\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecd850b",
   "metadata": {},
   "source": [
    "# Encoding Categorical Variables for Model Training\n",
    "\n",
    "In this step, we encode categorical variables into numerical values, which is a necessary preprocessing step for most machine learning algorithms. Categorical variables such as gender, extracurricular activities, and career aspirations are mapped to integers, making the data suitable for the machine learning model to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52d54762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "gender_map = {'male': 0, 'female': 1}\n",
    "extracurricular_activities_map = {False: 0, True: 1}\n",
    "career_aspiration_map = {\n",
    "    \"Bachelor of Statistics\": 0,\n",
    "    \"Bachelor of Supply Chain Management\": 1,\n",
    "    \"Bachelor of Corporate Communications\": 2,\n",
    "    \"Bachelor of Human Resouce Management\": 3,\n",
    "    \"Bachelor of Development Studies\": 4,\n",
    "    \"Bachelor of Procurement and Contract Management\": 5,\n",
    "    \"Bachelor of Project Management\": 6,\n",
    "    \"Bachelor of Business Administration\": 7,\n",
    "    \"Bachelor of Journalism\": 8,\n",
    "    \"Bachelor of Business and Office Management\": 9,\n",
    "    \"Bachelor of Economics and Statistics\": 10,\n",
    "    \"Bachelor of Mass Communication\": 11,\n",
    "    \"Bachelor of Commerce\": 12,\n",
    "    \"Bachelor of Procurement and Logistics\": 13,\n",
    "    \"Bachelor of Finance\": 14,\n",
    "    \"Bachelor of Business Information Technology\": 15,\n",
    "    \"Bachelor of Technology and Entrepreneurship Management\": 16,\n",
    "}\n",
    "# Apply mapping to the DataFrame\n",
    "df['gender'] = df['gender'].map(gender_map)\n",
    "df['extracurricular_activities'] = df['extracurricular_activities'].map(extracurricular_activities_map)\n",
    "df['career_aspiration'] = df['career_aspiration'].map(career_aspiration_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa8c23b",
   "metadata": {},
   "source": [
    "# Balance Dataset\n",
    "\n",
    "Before applying any balancing techniques, it’s important to understand the distribution of the target classes. In our case, if certain career aspirations are underrepresented, the model may become biased toward the more frequent classes.\n",
    "\n",
    "To address this issue, we use the Synthetic Minority Over-sampling Technique (SMOTE). Class imbalance arises when some career aspirations are underrepresented in the dataset, leading to potential bias in model predictions. By applying SMOTE, we generate synthetic samples for the minority classes, thereby creating a more balanced dataset. This approach enhances the model's performance and ensures fairness in the predictions across all career aspirations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7fd61cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  9, 10, 11, 12, 15, 13, 16,  8, 14],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the unique values in the 'career_aspiration' column to understand the diversity of career goals\n",
    "df[\"career_aspiration\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea02291",
   "metadata": {},
   "source": [
    "Our dataset includes 14 unique career aspirations, each representing a distinct course. This means that the recommendations generated by our model will be focused on these 14 courses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "473b122a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "career_aspiration\n",
       "5     315\n",
       "7     309\n",
       "4     223\n",
       "10    169\n",
       "0     138\n",
       "12    126\n",
       "1     119\n",
       "14     83\n",
       "8      73\n",
       "13     68\n",
       "3      67\n",
       "16     63\n",
       "2      61\n",
       "6      59\n",
       "15     56\n",
       "9      39\n",
       "11     32\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the number of instances for each unique value in 'career_aspiration' to identify any class imbalance\n",
    "df[\"career_aspiration\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a40e2e0",
   "metadata": {},
   "source": [
    "The distribution of career aspirations in our dataset reveals a significant imbalance among the various categories. \n",
    "\n",
    "- Career Aspirations 5 and 7 are the most prevalent, with 315 and 309 occurrences, respectively. This suggests that these career paths are the most popular or preferred among students.\n",
    "\n",
    "- Career Aspirations 11 and 9 are the least frequent, with only 32 and 39 occurrences each. This indicates that these career paths are less common and might be underrepresented in the dataset.\n",
    "\n",
    "- The other career aspirations fall somewhere in between, with varying counts that further highlight the disparity. For instance, aspirations like 4, 10, and 0 have counts of 223, 169, and 138, respectively, while aspirations like 16, 15, and 2 have counts as low as 63, 56, and 61.\n",
    "\n",
    "This imbalance could lead to challenges in model performance, as machine learning models tend to favor the majority classes, potentially neglecting the minority ones. To address this, it is crucial to apply balancing techniques, such as SMOTE, to ensure that each career aspiration is fairly represented in the training data. This will help in creating a model that can provide accurate and equitable recommendations across all career paths.\n",
    "\n",
    "We will apply SMOTE to the career aspirations in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6e9f715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the SMOTE class from the imbalanced-learn library\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Create a SMOTE object with a set random state for reproducibility\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "# Separate the features (X) and the target variable (y)\n",
    "X = df.drop(\n",
    "    \"career_aspiration\", axis=1\n",
    ")  # Features are all columns except 'career_aspiration'\n",
    "y = df[\"career_aspiration\"]  # Target variable is the 'career_aspiration' column\n",
    "\n",
    "# Apply SMOTE to balance the dataset by oversampling the minority classes\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a93694ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaNs in y: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of NaNs in y:\", y.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14149af8",
   "metadata": {},
   "source": [
    "# Train test Split\n",
    "\n",
    "we divide our balanced dataset into training and testing sets. The training set is used to train the model, while the testing set is used to assess how well the model generalizes to new, unseen examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2dd8b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the train_test_split function from sklearn to split the dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the balanced dataset into training and testing sets\n",
    "# 'test_size=0.2' means 20% of the data will be used for testing, and 80% for training\n",
    "# 'random_state=42' ensures reproducibility of the split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_resampled, y_resampled, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12984069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4284, 12)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the shape of the training feature set\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9a93ee",
   "metadata": {},
   "source": [
    "# Feature Scalling\n",
    "\n",
    "Feature scaling standardizes the range of independent variables or features. This process ensures that each feature contributes equally to the model, improving convergence and performance, particularly for algorithms that are sensitive to feature scaling, like support vector machines or gradient-based methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e08701dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the StandardScaler class from sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the training data and transform both training and testing data\n",
    "# 'fit_transform' calculates the mean and standard deviation from the training data and scales it\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "# 'transform' scales the testing data based on the mean and standard deviation of the training data\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37bcd1e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4284, 12)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scaled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9b5a5f",
   "metadata": {},
   "source": [
    "# Model Training and Evaluation\n",
    "\n",
    "We will train and evaluate several machine learning models to determine which one best fits our course recommender system. The selected models are Logistic Regression, Support Vector Classifier (SVC), Random Forest Classifier, Gradient Boosting Classifier, and XGBoost Classifier. \n",
    "\n",
    "## Logistic Regression\n",
    "\n",
    "### How It Works\n",
    "Logistic Regression models the probability of a binary outcome based on one or more predictor variables. For multi-class classification, it extends to multinomial logistic regression.\n",
    "\n",
    "#### Pros\n",
    "- Simple and interpretable.\n",
    "- Effective for linearly separable data.\n",
    "- Provides probabilities for class memberships.\n",
    "\n",
    "#### Cons\n",
    "- Assumes linear relationships between features and the target variable.\n",
    "- May not perform well with complex or non-linear data.\n",
    "- Can be sensitive to feature scaling.\n",
    "\n",
    "## Support Vector Classifier (SVC)\n",
    "\n",
    "### How It Work\n",
    "SVC finds the hyperplane that best separates the data into different classes. It uses kernel functions to handle non-linear data by transforming it into higher dimensions.\n",
    "\n",
    "#### Pros\n",
    "- Effective in high-dimensional spaces.\n",
    "- Works well for both linear and non-linear problems (with the appropriate kernel).\n",
    "\n",
    "#### Cons\n",
    "- Computationally intensive, especially with large datasets.\n",
    "- Requires careful tuning of parameters like the kernel and regularization.\n",
    "\n",
    "## Random Forest Classifier\n",
    "\n",
    "### How It Work\n",
    " \n",
    "Random Forest is an ensemble method that builds multiple decision trees and merges their predictions. It uses bagging (bootstrap aggregating) to improve performance and control overfitting.\n",
    "\n",
    "#### Pros\n",
    "- Handles both numerical and categorical features.\n",
    "- Robust to overfitting and provides feature importance scores.\n",
    "- Can manage imbalanced datasets effectively.\n",
    "\n",
    "#### Cons\n",
    "- Less interpretable due to the complexity of multiple trees.\n",
    "- Can be computationally expensive with a large number of trees.\n",
    "\n",
    "## Gradient Boosting Classifier\n",
    "\n",
    "### How It Works\n",
    " \n",
    "Gradient Boosting builds models sequentially, with each new model correcting the errors of the previous ones. It uses gradient descent to minimize the loss function.\n",
    "\n",
    "#### Pros\n",
    "- Often achieves high predictive accuracy.\n",
    "- Handles various types of data and relationships well.\n",
    "\n",
    "#### Cons\n",
    "- Prone to overfitting if not tuned properly.\n",
    "- Computationally expensive and requires careful parameter tuning.\n",
    "\n",
    "## XGBoost Classifier\n",
    "\n",
    "### How It Works\n",
    "XGBoost is a gradient boosting algorithm that incorporates advanced features like regularization and parallel processing. It builds an ensemble of trees in a boosting framework.\n",
    "\n",
    "#### Pros\n",
    "- Known for its speed and performance.\n",
    "- Handles missing values and imbalanced data effectively.\n",
    "- Offers various hyperparameters for fine-tuning.\n",
    "\n",
    "#### Cons\n",
    "- Requires careful tuning of hyperparameters.\n",
    "- Complexity can make it less interpretable.\n",
    "\n",
    "Each of these models has its strengths and weaknesses. By evaluating them using accuracy, classification reports, and confusion matrices, we can determine which model best balances performance and computational efficiency for our course recommender system. The goal is to choose a model that provides accurate and reliable recommendations, handling the complexity and diversity of student preferences effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49d8f73c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Model: Logistic Regression\n",
      "Accuracy: 0.34827264239028943\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.30      0.25      0.27        68\n",
      "           1       0.40      0.71      0.51        72\n",
      "           2       0.22      0.12      0.16        57\n",
      "           3       0.24      0.16      0.19        58\n",
      "           4       0.33      0.09      0.14        66\n",
      "           5       0.29      0.25      0.27        76\n",
      "           6       0.40      0.62      0.49        71\n",
      "           7       0.47      0.46      0.46        61\n",
      "           8       0.24      0.33      0.28        49\n",
      "           9       0.33      0.37      0.35        63\n",
      "          10       0.22      0.11      0.15        64\n",
      "          11       0.35      0.66      0.46        50\n",
      "          12       0.41      0.41      0.41        69\n",
      "          13       0.35      0.56      0.43        55\n",
      "          14       0.36      0.16      0.22        62\n",
      "          15       0.12      0.05      0.07        65\n",
      "          16       0.47      0.63      0.54        65\n",
      "\n",
      "    accuracy                           0.35      1071\n",
      "   macro avg       0.32      0.35      0.32      1071\n",
      "weighted avg       0.32      0.35      0.32      1071\n",
      "\n",
      "Confusion Matrix:\n",
      " [[17  6  2  3  0  4  5  1  4  2  2 11  3  3  0  3  2]\n",
      " [ 2 51  0  0  0  5  1  0  0  8  0  0  0  4  0  0  1]\n",
      " [ 3  2  7  4  0  1  2  4 10  3  0  7  4  7  0  2  1]\n",
      " [ 2  7  8  9  3  2  8  2  2  3  1  0  1  1  5  0  4]\n",
      " [ 7  7  1  3  6  5  5  5  2  2  2  1  3  4  1  7  5]\n",
      " [ 4  8  0  0  1 19  1  1  6  2 10  1  6 11  2  0  4]\n",
      " [ 2  0  0  4  0  2 44  3  0  1  2 10  1  0  0  0  2]\n",
      " [ 0  0  0  2  3  7  2 28  0  0  1  4  6  0  2  0  6]\n",
      " [ 3  3  1  2  1  4  0  1 16  3  3  0  7  2  1  2  0]\n",
      " [ 0 20  1  0  0  0  0  0  0 23  1 11  0  1  3  0  3]\n",
      " [ 7  4  0  3  0  8 13  1  3  1  7  3  4  4  0  2  4]\n",
      " [ 1  2  1  0  0  0  6  0  0  2  1 33  0  0  0  2  2]\n",
      " [ 0  7  1  0  1  5  3  2 10  0  1  1 28  6  1  2  1]\n",
      " [ 3  4  0  0  1  0  0  0  5  2  0  2  0 31  1  0  6]\n",
      " [ 2  1  3  4  1  2  8 10  6  0  1  3  6  1 10  3  1]\n",
      " [ 4  4  6  3  1  1  8  1  3 12  0  5  0  8  2  3  4]\n",
      " [ 0  3  1  0  0  1  3  1  0  6  0  3  0  6  0  0 41]]\n",
      "==================================================\n",
      "Model: Support Vector Classifier\n",
      "Accuracy: 0.5508870214752568\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.44      0.43        68\n",
      "           1       0.54      0.85      0.66        72\n",
      "           2       0.48      0.44      0.46        57\n",
      "           3       0.61      0.60      0.61        58\n",
      "           4       0.57      0.12      0.20        66\n",
      "           5       0.38      0.33      0.35        76\n",
      "           6       0.58      0.89      0.70        71\n",
      "           7       0.90      0.62      0.74        61\n",
      "           8       0.42      0.45      0.44        49\n",
      "           9       0.53      0.78      0.63        63\n",
      "          10       0.36      0.28      0.32        64\n",
      "          11       0.58      0.86      0.69        50\n",
      "          12       0.80      0.41      0.54        69\n",
      "          13       0.47      0.65      0.55        55\n",
      "          14       0.60      0.52      0.56        62\n",
      "          15       0.59      0.45      0.51        65\n",
      "          16       0.71      0.74      0.72        65\n",
      "\n",
      "    accuracy                           0.55      1071\n",
      "   macro avg       0.56      0.55      0.54      1071\n",
      "weighted avg       0.56      0.55      0.53      1071\n",
      "\n",
      "Confusion Matrix:\n",
      " [[30  5  3  1  1  2  5  0  0  3  4  8  0  4  0  1  1]\n",
      " [ 1 61  0  0  0  1  1  0  0  6  0  0  0  2  0  0  0]\n",
      " [ 4  1 25  3  0  1  3  0  8  3  1  6  0  1  0  1  0]\n",
      " [ 1  7  3 35  2  2  1  0  2  1  1  0  0  1  1  1  0]\n",
      " [ 5  6  0  2  8  5  7  0  1  2  5  3  3  3  6  7  3]\n",
      " [ 2  9  5  2  0 25  2  1  5  3  8  0  1  9  1  1  2]\n",
      " [ 2  0  0  0  0  1 63  0  0  0  3  2  0  0  0  0  0]\n",
      " [ 1  0  1  4  0  6  1 38  0  0  1  1  1  2  2  1  2]\n",
      " [ 6  2  3  0  1  4  0  0 22  1  6  0  0  4  0  0  0]\n",
      " [ 0 10  0  0  0  0  0  0  0 49  0  2  0  0  1  0  1]\n",
      " [ 9  0  0  1  1  3 10  0  3  4 18  3  1  3  3  3  2]\n",
      " [ 0  0  1  0  0  0  4  0  0  0  0 43  0  0  0  0  2]\n",
      " [ 2  4  5  1  1  5  1  0  5  5  3  1 28  4  4  0  0]\n",
      " [ 1  3  0  1  0  0  0  0  3  4  0  0  0 36  1  2  4]\n",
      " [ 1  1  2  6  0  8  5  2  2  0  0  1  1  0 32  1  0]\n",
      " [ 5  4  4  1  0  0  3  0  0  6  0  3  0  5  2 29  3]\n",
      " [ 0  0  0  0  0  2  2  1  1  6  0  1  0  2  0  2 48]]\n",
      "==================================================\n",
      "Model: Random Forest Classifier\n",
      "Accuracy: 0.7927170868347339\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.78      0.73        68\n",
      "           1       0.82      0.96      0.88        72\n",
      "           2       0.88      0.88      0.88        57\n",
      "           3       0.76      0.91      0.83        58\n",
      "           4       0.85      0.42      0.57        66\n",
      "           5       0.47      0.21      0.29        76\n",
      "           6       0.83      0.92      0.87        71\n",
      "           7       0.98      0.72      0.83        61\n",
      "           8       0.79      0.94      0.86        49\n",
      "           9       0.84      0.97      0.90        63\n",
      "          10       0.63      0.72      0.67        64\n",
      "          11       0.86      0.98      0.92        50\n",
      "          12       0.89      0.72      0.80        69\n",
      "          13       0.70      0.98      0.82        55\n",
      "          14       0.85      0.82      0.84        62\n",
      "          15       0.85      0.88      0.86        65\n",
      "          16       0.79      0.88      0.83        65\n",
      "\n",
      "    accuracy                           0.79      1071\n",
      "   macro avg       0.79      0.81      0.79      1071\n",
      "weighted avg       0.79      0.79      0.78      1071\n",
      "\n",
      "Confusion Matrix:\n",
      " [[53  4  0  1  2  1  0  0  2  0  1  0  1  1  0  1  1]\n",
      " [ 0 69  0  0  0  0  0  0  0  3  0  0  0  0  0  0  0]\n",
      " [ 0  0 50  0  0  1  2  0  1  0  1  2  0  0  0  0  0]\n",
      " [ 1  3  0 53  0  0  0  0  0  0  0  0  0  0  0  0  1]\n",
      " [ 5  3  2  2 28  4  2  0  2  3  5  2  0  0  2  4  2]\n",
      " [ 5  3  3  3  1 16  2  0  6  4 12  0  3 10  1  1  6]\n",
      " [ 1  0  1  0  0  0 65  0  0  0  2  1  1  0  0  0  0]\n",
      " [ 2  0  0  4  0  2  2 44  0  0  2  0  0  2  1  0  2]\n",
      " [ 2  0  0  0  0  0  0  0 46  0  1  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 61  0  0  0  0  0  1  1]\n",
      " [ 4  0  1  4  0  1  1  0  1  1 46  2  1  0  2  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0 49  0  0  0  0  1]\n",
      " [ 2  0  0  0  1  4  1  0  0  0  2  0 50  6  1  2  0]\n",
      " [ 1  0  0  0  0  0  0  0  0  0  0  0  0 54  0  0  0]\n",
      " [ 0  1  0  2  1  3  1  1  0  0  1  0  0  0 51  1  0]\n",
      " [ 0  1  0  0  0  0  2  0  0  0  0  1  0  1  2 57  1]\n",
      " [ 1  0  0  1  0  2  0  0  0  1  0  0  0  3  0  0 57]]\n",
      "==================================================\n",
      "Model: K Nearest Neighbors\n",
      "Accuracy: 0.6414565826330533\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.66      0.62        68\n",
      "           1       0.77      0.89      0.83        72\n",
      "           2       0.62      0.88      0.72        57\n",
      "           3       0.53      0.71      0.60        58\n",
      "           4       0.32      0.15      0.21        66\n",
      "           5       0.32      0.12      0.17        76\n",
      "           6       0.77      0.87      0.82        71\n",
      "           7       0.81      0.28      0.41        61\n",
      "           8       0.50      0.67      0.57        49\n",
      "           9       0.69      0.92      0.79        63\n",
      "          10       0.56      0.48      0.52        64\n",
      "          11       0.70      0.98      0.82        50\n",
      "          12       0.72      0.48      0.57        69\n",
      "          13       0.63      0.87      0.73        55\n",
      "          14       0.68      0.55      0.61        62\n",
      "          15       0.68      0.82      0.74        65\n",
      "          16       0.77      0.77      0.77        65\n",
      "\n",
      "    accuracy                           0.64      1071\n",
      "   macro avg       0.63      0.65      0.62      1071\n",
      "weighted avg       0.63      0.64      0.61      1071\n",
      "\n",
      "Confusion Matrix:\n",
      " [[45  2  0  2  2  2  0  0  4  0  2  6  0  0  0  2  1]\n",
      " [ 0 64  0  0  0  1  1  0  2  1  1  0  0  1  0  1  0]\n",
      " [ 1  0 50  0  1  0  1  0  1  0  1  1  0  0  0  1  0]\n",
      " [ 2  4  1 41  3  1  0  0  1  1  1  0  0  0  0  0  3]\n",
      " [ 5  3  7  2 10  2  6  1  5  6  3  1  4  2  4  4  1]\n",
      " [ 8  4  6  5  3  9  0  0  8  4  6  2  2  9  3  3  4]\n",
      " [ 0  0  1  4  0  0 62  0  0  0  3  0  1  0  0  0  0]\n",
      " [ 2  1  3  4  7  1  0 17  4  0  4  1  1  5  2  6  3]\n",
      " [ 1  1  4  3  0  1  0  0 33  1  1  0  0  3  1  0  0]\n",
      " [ 0  1  0  1  0  0  0  0  0 58  0  1  0  1  0  1  0]\n",
      " [ 6  0  2  2  1  4  1  0  2  1 31  3  3  1  3  2  2]\n",
      " [ 1  0  0  0  0  0  0  0  0  0  0 49  0  0  0  0  0]\n",
      " [ 2  1  4  3  4  4  3  0  3  3  0  1 33  5  2  1  0]\n",
      " [ 1  0  0  1  0  0  0  0  0  0  1  0  0 48  1  2  1]\n",
      " [ 3  0  3  7  0  3  4  1  1  2  0  1  2  1 34  0  0]\n",
      " [ 1  2  0  0  0  0  2  2  0  2  0  3  0  0  0 53  0]\n",
      " [ 0  0  0  3  0  0  1  0  2  5  1  1  0  0  0  2 50]]\n",
      "==================================================\n",
      "Model: Decision Tree Classifier\n",
      "Accuracy: 0.6078431372549019\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.53      0.58        68\n",
      "           1       0.72      0.82      0.77        72\n",
      "           2       0.58      0.56      0.57        57\n",
      "           3       0.61      0.67      0.64        58\n",
      "           4       0.35      0.30      0.33        66\n",
      "           5       0.18      0.16      0.17        76\n",
      "           6       0.77      0.76      0.77        71\n",
      "           7       0.64      0.61      0.62        61\n",
      "           8       0.49      0.57      0.53        49\n",
      "           9       0.79      0.79      0.79        63\n",
      "          10       0.51      0.48      0.50        64\n",
      "          11       0.68      0.88      0.77        50\n",
      "          12       0.66      0.55      0.60        69\n",
      "          13       0.71      0.84      0.77        55\n",
      "          14       0.62      0.58      0.60        62\n",
      "          15       0.54      0.58      0.56        65\n",
      "          16       0.77      0.78      0.78        65\n",
      "\n",
      "    accuracy                           0.61      1071\n",
      "   macro avg       0.60      0.62      0.61      1071\n",
      "weighted avg       0.60      0.61      0.60      1071\n",
      "\n",
      "Confusion Matrix:\n",
      " [[36  1  2  2  0  6  0  1  3  1  2  4  3  1  1  5  0]\n",
      " [ 1 59  0  0  0  1  0  1  0  1  1  0  2  2  0  2  2]\n",
      " [ 1  1 32  2  3  2  2  1  1  1  1  3  1  2  4  0  0]\n",
      " [ 0  3  2 39  3  2  1  1  1  2  2  0  0  1  1  0  0]\n",
      " [ 2  3  3  1 20  9  1  4  4  0  4  2  0  1  3  6  3]\n",
      " [ 4  3  3  4  6 12  3  4  5  5  9  3  4  3  3  2  3]\n",
      " [ 0  1  2  1  4  1 54  0  1  0  3  0  0  0  1  2  1]\n",
      " [ 1  0  0  5  7  3  0 37  3  0  0  1  1  1  2  0  0]\n",
      " [ 1  1  5  1  1  5  1  0 28  0  2  0  1  1  1  0  1]\n",
      " [ 1  3  0  0  0  2  0  0  0 50  1  0  2  0  0  3  1]\n",
      " [ 2  1  1  1  2  5  3  1  3  0 31  7  3  0  1  1  2]\n",
      " [ 1  0  2  0  1  1  0  0  1  0  0 44  0  0  0  0  0]\n",
      " [ 1  1  1  0  0  3  1  5  3  1  3  0 38  5  2  5  0]\n",
      " [ 1  1  1  0  0  1  0  0  1  1  0  0  0 46  2  0  1]\n",
      " [ 1  0  0  2  8  4  2  2  1  0  1  0  2  0 36  3  0]\n",
      " [ 3  3  1  4  2  4  2  0  2  1  0  1  1  1  1 38  1]\n",
      " [ 0  1  0  2  0  4  0  1  0  0  1  0  0  1  0  4 51]]\n",
      "==================================================\n",
      "Model: Gaussian Naive Bayes\n",
      "Accuracy: 0.19794584500466852\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.14      0.01      0.03        68\n",
      "           1       0.53      0.28      0.36        72\n",
      "           2       0.00      0.00      0.00        57\n",
      "           3       0.60      0.05      0.10        58\n",
      "           4       0.57      0.06      0.11        66\n",
      "           5       0.50      0.09      0.16        76\n",
      "           6       0.19      0.97      0.31        71\n",
      "           7       0.51      0.36      0.42        61\n",
      "           8       0.12      0.02      0.04        49\n",
      "           9       0.00      0.00      0.00        63\n",
      "          10       0.00      0.00      0.00        64\n",
      "          11       0.70      0.28      0.40        50\n",
      "          12       1.00      0.07      0.14        69\n",
      "          13       0.10      0.98      0.18        55\n",
      "          14       0.58      0.11      0.19        62\n",
      "          15       0.56      0.08      0.14        65\n",
      "          16       0.00      0.00      0.00        65\n",
      "\n",
      "    accuracy                           0.20      1071\n",
      "   macro avg       0.36      0.20      0.15      1071\n",
      "weighted avg       0.36      0.20      0.15      1071\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 1  0  1  0  0  0 21  0  0  0  0  1  0 44  0  0  0]\n",
      " [ 0 20  0  0  0  0 17  0  0  0  0  0  0 35  0  0  0]\n",
      " [ 0  0  0  0  1  0 12  3  3  0  0  1  0 37  0  0  0]\n",
      " [ 0  0  1  3  1  0 23  3  0  0  0  0  0 27  0  0  0]\n",
      " [ 0  2  0  0  4  1 22  2  1  0  0  0  0 29  2  3  0]\n",
      " [ 0  2  0  0  0  7 27  1  1  0  0  0  0 37  0  1  0]\n",
      " [ 0  0  0  0  0  0 69  2  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  2  0  1 21 22  0  0  0  0  0 12  3  0  0]\n",
      " [ 0  0  0  0  0  2  5  3  1  0  0  0  0 38  0  0  0]\n",
      " [ 0  6  0  0  0  0 15  0  0  0  0  0  0 42  0  0  0]\n",
      " [ 0  2  0  0  1  1 31  0  0  0  0  0  0 29  0  0  0]\n",
      " [ 2  1  1  0  0  0 14  0  0  0  0 14  0 18  0  0  0]\n",
      " [ 0  0  0  0  0  1 19  2  1  0  0  0  5 41  0  0  0]\n",
      " [ 0  1  0  0  0  0  0  0  0  0  0  0  0 54  0  0  0]\n",
      " [ 1  1  0  0  0  1 30  5  0  0  0  1  0 16  7  0  0]\n",
      " [ 3  3  1  0  0  0 19  0  1  0  0  3  0 30  0  5  0]\n",
      " [ 0  0  0  0  0  0 24  0  0  0  0  0  0 41  0  0  0]]\n",
      "==================================================\n",
      "Model: AdaBoost Classifier\n",
      "Accuracy: 0.1839402427637722\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.04      0.08        68\n",
      "           1       0.64      0.10      0.17        72\n",
      "           2       0.00      0.00      0.00        57\n",
      "           3       0.23      0.09      0.12        58\n",
      "           4       0.24      0.06      0.10        66\n",
      "           5       0.14      0.05      0.08        76\n",
      "           6       0.00      0.00      0.00        71\n",
      "           7       0.16      0.87      0.27        61\n",
      "           8       0.15      0.24      0.19        49\n",
      "           9       0.17      0.62      0.27        63\n",
      "          10       0.20      0.02      0.03        64\n",
      "          11       0.19      0.42      0.26        50\n",
      "          12       1.00      0.09      0.16        69\n",
      "          13       0.27      0.29      0.28        55\n",
      "          14       0.27      0.13      0.17        62\n",
      "          15       0.00      0.00      0.00        65\n",
      "          16       0.15      0.28      0.20        65\n",
      "\n",
      "    accuracy                           0.18      1071\n",
      "   macro avg       0.25      0.19      0.14      1071\n",
      "weighted avg       0.26      0.18      0.13      1071\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 3  0  0  1  0  2  0 17 11 23  0  8  0  0  1  0  2]\n",
      " [ 0  7  0  5  0  2  0  3  0 21  0  1  0  1  0  0 32]\n",
      " [ 1  0  0  1  4  1  0 16  8 14  0  4  0  3  2  0  3]\n",
      " [ 0  3  0  5  0  0  0 28  5  5  0  5  0  0  1  0  6]\n",
      " [ 1  0  1  0  4  0  0 25  7  9  1  7  0  1  3  0  7]\n",
      " [ 1  0  0  2  0  4  0 14  8 15  1  9  0  9  0  0 13]\n",
      " [ 0  0  0  3  0  4  0 35  5  2  1 11  0  0  5  0  5]\n",
      " [ 0  0  0  1  0  1  0 53  0  0  0  1  0  1  2  0  2]\n",
      " [ 0  0  0  0  3  0  0 13 12  8  1  3  0  6  1  0  2]\n",
      " [ 0  0  0  0  1  1  0  0  0 39  0 14  0  1  0  0  7]\n",
      " [ 0  0  2  1  0  5  0 16  6 15  1 11  0  1  1  0  5]\n",
      " [ 0  0  0  0  0  0  1 22  0  6  0 21  0  0  0  0  0]\n",
      " [ 0  0  4  0  3  4  0 31  3 12  0  4  6  1  0  0  1]\n",
      " [ 0  0  1  1  0  0  0  4  0 33  0  0  0 16  0  0  0]\n",
      " [ 1  0  2  1  1  2  0 34  5  0  0  2  0  2  8  0  4]\n",
      " [ 0  0  2  0  1  1  0 16  8 12  0  6  0  4  5  0 10]\n",
      " [ 0  1  0  1  0  2  0  6  0 16  0  6  0 14  1  0 18]]\n",
      "==================================================\n",
      "Model: Gradient Boosting Classifier\n",
      "Accuracy: 0.6685340802987861\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.57      0.53        68\n",
      "           1       0.78      0.94      0.86        72\n",
      "           2       0.72      0.67      0.69        57\n",
      "           3       0.67      0.74      0.70        58\n",
      "           4       0.43      0.15      0.22        66\n",
      "           5       0.44      0.33      0.38        76\n",
      "           6       0.71      0.89      0.79        71\n",
      "           7       0.98      0.84      0.90        61\n",
      "           8       0.65      0.65      0.65        49\n",
      "           9       0.74      0.92      0.82        63\n",
      "          10       0.38      0.39      0.38        64\n",
      "          11       0.71      0.92      0.80        50\n",
      "          12       0.83      0.57      0.67        69\n",
      "          13       0.58      0.89      0.70        55\n",
      "          14       0.80      0.52      0.63        62\n",
      "          15       0.70      0.74      0.72        65\n",
      "          16       0.74      0.77      0.75        65\n",
      "\n",
      "    accuracy                           0.67      1071\n",
      "   macro avg       0.67      0.68      0.66      1071\n",
      "weighted avg       0.66      0.67      0.65      1071\n",
      "\n",
      "Confusion Matrix:\n",
      " [[39  4  0  1  2  4  1  0  2  1  5  2  2  1  0  3  1]\n",
      " [ 0 68  0  0  0  0  0  0  0  2  1  0  0  1  0  0  0]\n",
      " [ 1  2 38  1  0  1  3  0  1  1  1  3  0  2  0  2  1]\n",
      " [ 3  2  2 43  2  0  2  0  0  1  2  0  0  0  0  0  1]\n",
      " [ 6  2  3  3 10  5  3  0  1  2  8  2  1  3  6  6  5]\n",
      " [ 5  3  0  2  2 25  4  0  6  3  6  1  1 13  0  2  3]\n",
      " [ 1  0  0  0  0  0 63  0  0  0  5  2  0  0  0  0  0]\n",
      " [ 1  0  0  2  1  2  3 51  0  0  0  0  1  0  0  0  0]\n",
      " [ 2  0  0  2  0  3  0  0 32  0  4  0  1  3  2  0  0]\n",
      " [ 0  2  0  0  0  0  0  0  0 58  0  1  0  1  0  0  1]\n",
      " [10  1  2  1  1  5  3  0  2  2 25  5  0  3  0  3  1]\n",
      " [ 2  0  0  0  0  0  0  0  0  0  0 46  0  0  0  0  2]\n",
      " [ 1  2  3  2  0  1  1  1  2  4  5  1 39  4  0  2  1]\n",
      " [ 0  0  1  0  1  1  0  0  0  1  0  0  1 49  0  0  1]\n",
      " [ 3  1  2  5  3  5  3  0  2  0  3  1  0  1 32  1  0]\n",
      " [ 3  0  2  1  1  2  3  0  0  1  0  1  1  1  0 48  1]\n",
      " [ 2  0  0  1  0  3  0  0  1  2  1  0  0  3  0  2 50]]\n",
      "==================================================\n",
      "Model: XGBoost Classifier\n",
      "Accuracy: 0.780578898225957\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.72      0.68        68\n",
      "           1       0.89      0.93      0.91        72\n",
      "           2       0.85      0.88      0.86        57\n",
      "           3       0.84      0.83      0.83        58\n",
      "           4       0.69      0.38      0.49        66\n",
      "           5       0.42      0.33      0.37        76\n",
      "           6       0.86      0.87      0.87        71\n",
      "           7       0.96      0.79      0.86        61\n",
      "           8       0.74      0.86      0.79        49\n",
      "           9       0.91      0.98      0.95        63\n",
      "          10       0.56      0.67      0.61        64\n",
      "          11       0.86      0.98      0.92        50\n",
      "          12       0.80      0.71      0.75        69\n",
      "          13       0.75      1.00      0.86        55\n",
      "          14       0.88      0.81      0.84        62\n",
      "          15       0.81      0.89      0.85        65\n",
      "          16       0.84      0.83      0.84        65\n",
      "\n",
      "    accuracy                           0.78      1071\n",
      "   macro avg       0.78      0.79      0.78      1071\n",
      "weighted avg       0.78      0.78      0.77      1071\n",
      "\n",
      "Confusion Matrix:\n",
      " [[49  0  0  2  1  4  0  0  1  0  4  0  3  1  0  1  2]\n",
      " [ 0 67  0  0  0  1  0  0  0  1  1  0  1  0  0  1  0]\n",
      " [ 1  0 50  0  0  2  2  0  0  0  0  1  0  0  0  1  0]\n",
      " [ 2  2  1 48  2  0  0  1  0  0  0  0  1  0  0  0  1]\n",
      " [ 7  2  2  3 25  4  2  0  2  0  4  2  1  1  4  5  2]\n",
      " [ 7  3  2  0  1 25  1  0  7  2  9  1  2 10  2  0  4]\n",
      " [ 0  0  0  0  0  2 62  0  0  0  6  0  0  0  0  1  0]\n",
      " [ 1  0  0  3  3  2  2 48  0  0  0  0  2  0  0  0  0]\n",
      " [ 1  0  0  0  0  3  0  0 42  0  2  0  1  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 62  0  1  0  0  0  0  0]\n",
      " [ 6  0  1  1  1  4  1  0  3  0 43  2  0  1  0  1  0]\n",
      " [ 1  0  0  0  0  0  0  0  0  0  0 49  0  0  0  0  0]\n",
      " [ 1  0  1  0  0  5  0  0  1  3  5  0 49  2  0  2  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0 55  0  0  0]\n",
      " [ 0  1  2  0  3  1  0  1  0  0  3  0  0  0 50  1  0]\n",
      " [ 0  0  0  0  0  2  2  0  0  0  0  1  1  0  0 58  1]\n",
      " [ 1  0  0  0  0  4  0  0  1  0  0  0  0  3  1  1 54]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"Support Vector Classifier\": SVC(),\n",
    "    \"Random Forest Classifier\": RandomForestClassifier(),\n",
    "    \"Gradient Boosting Classifier\": GradientBoostingClassifier(),\n",
    "    \"XGBoost Classifier\": XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
    "}\n",
    "\n",
    "# Train and evaluate each model\n",
    "for name, model in models.items():\n",
    "    print(\"=\"*50)\n",
    "    print(\"Model:\", name)\n",
    "    # Train the model\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Predict on test set\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    classification_rep = classification_report(y_test, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Print metrics\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Classification Report:\\n\", classification_rep)\n",
    "    print(\"Confusion Matrix:\\n\", conf_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f69bafe",
   "metadata": {},
   "source": [
    "# summary of the model performance results\n",
    "\n",
    "## Logistic Regression\n",
    "\n",
    "- Accuracy: 34.83%\n",
    "- Classification Report:\n",
    "- Precision: Ranges from 0.12 to 0.47 across classes.\n",
    "- Recall: Ranges from 0.05 to 0.71 across classes.\n",
    "- F1-Score: Ranges from 0.07 to 0.54 across classes.\n",
    "- Confusion Matrix: Indicates significant misclassification across most classes, with better performance for class 1 and class 6.\n",
    "\n",
    "## Support Vector Classifier (SVC)\n",
    "- Accuracy: 55.09%\n",
    "- Classification Report:\n",
    "- Precision: Ranges from 0.36 to 0.90 across classes.\n",
    "- Recall: Ranges from 0.28 to 0.89 across classes.\n",
    "- F1-Score: Ranges from 0.32 to 0.74 across classes.\n",
    "- Confusion Matrix: Shows improved performance over Logistic Regression, particularly for classes 1, 6, and 7.\n",
    "\n",
    "## Random Forest Classifier\n",
    "- Accuracy: 79.27%\n",
    "- Classification Report:\n",
    "- Precision: Ranges from 0.47 to 0.98 across classes.\n",
    "- Recall: Ranges from 0.21 to 0.92 across classes.\n",
    "- F1-Score: Ranges from 0.29 to 0.90 across classes.\n",
    "- Confusion Matrix: Demonstrates strong performance across most classes, especially for classes 1, 6, and 7.\n",
    "\n",
    "## Gradient Boosting Classifier\n",
    "- Accuracy: 66.85%\n",
    "- Classification Report:\n",
    "- Precision: Ranges from 0.43 to 0.98 across classes.\n",
    "- Recall: Ranges from 0.15 to 0.89 across classes.\n",
    "- F1-Score: Ranges from 0.22 to 0.90 across classes.\n",
    "- Confusion Matrix: Generally better than Logistic Regression and comparable to SVC, with particularly strong performance for classes 1, 7, and 9.\n",
    "\n",
    "### Summary \n",
    "- Random Forest Classifier shows the best overall accuracy and balanced performance across classes. It is the most robust model in this comparison.\n",
    "- Gradient Boosting Classifier also performs well, with significant accuracy and precision improvements over Logistic Regression.\n",
    "- Support Vector Classifier offers good performance but slightly less consistent across all classes compared to Random Forest and Gradient Boosting.\n",
    "Given these results, the Random Forest Classifier is the most reliable model for your task. It achieves the highest accuracy and performs well across most classes. \n",
    "We will tune this model further to potentially enhance performance even more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "63e6b1a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 324 candidates, totalling 1620 fits\n",
      "Best parameters found:  {'max_depth': 30, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    \"n_estimators\": [50, 100, 200],\n",
    "    \"max_depth\": [None, 10, 20, 30],\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "    \"min_samples_leaf\": [1, 2, 4],\n",
    "    \"max_features\": [\"auto\", \"sqrt\", \"log2\"],\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2\n",
    ")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters found: \", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "904a37e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.75      0.72        68\n",
      "           1       0.86      0.94      0.90        72\n",
      "           2       0.89      0.86      0.88        57\n",
      "           3       0.85      0.90      0.87        58\n",
      "           4       0.74      0.39      0.51        66\n",
      "           5       0.39      0.21      0.27        76\n",
      "           6       0.82      0.92      0.87        71\n",
      "           7       0.90      0.74      0.81        61\n",
      "           8       0.78      0.94      0.85        49\n",
      "           9       0.81      0.95      0.88        63\n",
      "          10       0.63      0.70      0.67        64\n",
      "          11       0.82      0.98      0.89        50\n",
      "          12       0.86      0.72      0.79        69\n",
      "          13       0.69      0.98      0.81        55\n",
      "          14       0.83      0.77      0.80        62\n",
      "          15       0.86      0.91      0.88        65\n",
      "          16       0.77      0.85      0.81        65\n",
      "\n",
      "    accuracy                           0.78      1071\n",
      "   macro avg       0.78      0.80      0.78      1071\n",
      "weighted avg       0.77      0.78      0.77      1071\n",
      "\n",
      "[[51  3  0  0  2  1  0  0  3  1  2  1  1  1  0  1  1]\n",
      " [ 0 68  0  0  0  1  0  0  0  2  0  0  1  0  0  0  0]\n",
      " [ 1  0 49  0  0  1  2  0  0  1  1  1  0  0  1  0  0]\n",
      " [ 1  2  1 52  0  0  0  0  0  0  0  0  1  0  0  0  1]\n",
      " [ 5  2  1  1 26  2  4  2  2  2  5  3  1  1  3  4  2]\n",
      " [ 6  2  2  3  1 16  3  0  6  4 10  0  2 11  2  1  7]\n",
      " [ 1  0  1  0  0  1 65  0  0  0  3  0  0  0  0  0  0]\n",
      " [ 2  0  0  3  0  3  1 45  1  0  1  0  0  2  1  1  1]\n",
      " [ 2  0  0  0  0  0  0  0 46  0  1  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 60  0  1  0  0  0  1  1]\n",
      " [ 3  0  1  1  2  2  2  0  1  0 45  4  1  0  1  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0 49  0  0  0  0  1]\n",
      " [ 1  1  0  0  1  6  0  0  0  1  1  0 50  5  1  2  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0 54  1  0  0]\n",
      " [ 0  0  0  1  2  5  0  3  0  0  1  1  1  0 48  0  0]\n",
      " [ 0  1  0  0  1  0  2  0  0  0  0  0  0  1  0 59  1]\n",
      " [ 0  0  0  0  0  3  0  0  0  3  1  0  0  3  0  0 55]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "# Create the model with best parameters\n",
    "model = RandomForestClassifier(\n",
    "    max_depth=30,\n",
    "    max_features=\"log2\",\n",
    "    min_samples_leaf=1,\n",
    "    min_samples_split=3,\n",
    "    n_estimators=200,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cf1ebd",
   "metadata": {},
   "source": [
    "# Untuned Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abccc43f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "406327da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.7889822595704948\n",
      "Report:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.78      0.76        68\n",
      "           1       0.79      0.94      0.86        72\n",
      "           2       0.84      0.84      0.84        57\n",
      "           3       0.85      0.86      0.85        58\n",
      "           4       0.73      0.41      0.52        66\n",
      "           5       0.45      0.24      0.31        76\n",
      "           6       0.85      0.93      0.89        71\n",
      "           7       0.91      0.70      0.80        61\n",
      "           8       0.78      0.96      0.86        49\n",
      "           9       0.83      0.95      0.89        63\n",
      "          10       0.66      0.75      0.70        64\n",
      "          11       0.86      1.00      0.93        50\n",
      "          12       0.89      0.71      0.79        69\n",
      "          13       0.70      1.00      0.82        55\n",
      "          14       0.83      0.79      0.81        62\n",
      "          15       0.84      0.86      0.85        65\n",
      "          16       0.79      0.89      0.84        65\n",
      "\n",
      "    accuracy                           0.79      1071\n",
      "   macro avg       0.79      0.80      0.78      1071\n",
      "weighted avg       0.78      0.79      0.78      1071\n",
      "\n",
      "Confusion Matrix:  [[53  3  2  1  1  0  0  0  1  1  2  1  0  1  0  1  1]\n",
      " [ 0 68  0  0  0  1  0  0  0  1  0  0  2  0  0  0  0]\n",
      " [ 2  0 48  0  1  1  2  0  1  0  1  1  0  0  0  0  0]\n",
      " [ 0  3  1 50  1  0  0  0  0  0  2  0  0  0  0  0  1]\n",
      " [ 5  3  1  3 27  5  1  0  2  2  5  2  0  1  3  4  2]\n",
      " [ 2  5  2  1  3 18  2  0  5  5 11  0  2 10  3  1  6]\n",
      " [ 1  0  1  0  0  0 66  0  0  0  2  0  1  0  0  0  0]\n",
      " [ 2  0  0  1  2  3  3 43  1  0  0  0  0  2  1  1  2]\n",
      " [ 2  0  0  0  0  0  0  0 47  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 60  0  1  0  0  0  1  1]\n",
      " [ 2  1  2  1  0  4  1  0  1  1 48  2  0  0  1  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0 50  0  0  0  0  0]\n",
      " [ 1  1  0  0  0  5  0  1  1  0  1  0 49  6  1  3  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0 55  0  0  0]\n",
      " [ 0  1  0  2  1  2  1  3  0  0  1  1  1  0 49  0  0]\n",
      " [ 0  1  0  0  1  0  2  0  1  0  0  0  0  1  1 56  2]\n",
      " [ 1  0  0  0  0  1  0  0  0  2  0  0  0  3  0  0 58]]\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier()\n",
    "\n",
    "model.fit(X_train_scaled, y_train)\n",
    "# Predict on test set\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate metrics\n",
    "print(\"Accuracy: \",accuracy_score(y_test, y_pred))\n",
    "print(\"Report: \",classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix: \",confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2297114b",
   "metadata": {},
   "source": [
    "# Single Input Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c6ff0f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Label: 12\n",
      "Model Prediction: 12\n",
      "The model's prediction is accurate.\n"
     ]
    }
   ],
   "source": [
    "# Test sample index\n",
    "test_index = 10\n",
    "\n",
    "# Print actual label and model prediction\n",
    "print(f\"Actual Label: {y_test.iloc[test_index]}\")\n",
    "prediction = model.predict(X_test_scaled[test_index].reshape(1, -1))[0]\n",
    "print(f\"Model Prediction: {prediction}\")\n",
    "\n",
    "# Check if prediction matches the actual label\n",
    "if y_test.iloc[test_index] == prediction:\n",
    "    print(\"The model's prediction is accurate.\")\n",
    "else:\n",
    "    print(\"The model's prediction is inaccurate.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0502b39b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Label : 0\n",
      "Model Prediction : 0\n",
      "Wow! Model doing well.....\n"
     ]
    }
   ],
   "source": [
    "# test 2\n",
    "print(\"Actual Label :\", y_test.iloc[300])\n",
    "print(\"Model Prediction :\",model.predict(X_test_scaled[300].reshape(1,-1))[0])\n",
    "if y_test.iloc[10]==model.predict(X_test_scaled[10].reshape(1,-1)):\n",
    "    print(\"The model's prediction is accurate.\")\n",
    "else:\n",
    "    print(\"The model's prediction is inaccurate.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28bdd4af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Label : 3\n",
      "Model Prediction : 3\n",
      "Wow! Model doing well.....\n"
     ]
    }
   ],
   "source": [
    "# test 2\n",
    "print(\"Actual Label :\", y_test.iloc[23])\n",
    "print(\"Model Prediction :\",model.predict(X_test_scaled[23].reshape(1,-1))[0])\n",
    "if y_test.iloc[10]==model.predict(X_test_scaled[10].reshape(1,-1)):\n",
    "    print(\"The model's prediction is accurate.\")\n",
    "else:\n",
    "    print(\"The model's prediction is inaccurate.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1cc97f",
   "metadata": {},
   "source": [
    "# Saving & Load Files\n",
    "\n",
    "we save the trained scaler and model using the `pickle` module to ensure they can be easily loaded and reused for future predictions. We chose pickle because it efficiently serializes Python objects, allowing us to preserve the exact preprocessing steps and model parameters. This approach ensures consistency in predictions, as the same scaling and model configurations are applied to any new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c88ef663",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pickle\n",
    "\n",
    "# Save the scaler and the model\n",
    "with open(\"Models/scaler.pkl\", \"wb\") as scaler_file:\n",
    "    pickle.dump(scaler, scaler_file)\n",
    "with open(\"Models/model.pkl\", \"wb\") as model_file:\n",
    "    pickle.dump(model, model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0238be1",
   "metadata": {},
   "source": [
    "# Recommendation System\n",
    "\n",
    "This script integrates the pre-trained Random Forest model and the scaler, both saved using pickle, to recommend suitable university courses for students based on various input features. The Recommendations function encodes the input features, scales them, and then uses the model to predict the top five recommended courses with their associated probabilities. This approach ensures that the predictions are consistent and aligned with the original training conditions, allowing for accurate and reliable recommendations. The model’s capability to output probabilities (predict_proba) is verified to ensure that the recommendations are backed by probabilistic reasoning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eff89839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.ensemble._forest.RandomForestClassifier'>\n",
      "Model supports predict_proba\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# Load the scaler, label encoder, model, and class names\n",
    "scaler = pickle.load(open(\"Models/scaler.pkl\", 'rb'))\n",
    "model = pickle.load(open(\"Models/model.pkl\", 'rb'))\n",
    "\n",
    "# Verify the model type\n",
    "print(\n",
    "    type(model)\n",
    ")  # Should output something like <class 'sklearn.ensemble._forest.RandomForestClassifier'>\n",
    "\n",
    "# Check if model supports predict_proba\n",
    "if hasattr(model, \"predict_proba\"):\n",
    "    print(\"Model supports predict_proba\")\n",
    "else:\n",
    "    print(\"Model does NOT support predict_proba\")\n",
    "    \n",
    "class_names = [\n",
    "    \"Bachelor of Supply Chain Management\",\n",
    "    \"Bachelor of Statistics\",\n",
    "    \"Bachelor of Corporate Communications\",\n",
    "    \"Bachelor of Human Resouce Management\",\n",
    "    \"Bachelor of Development Studies\",\n",
    "    \"Bachelor of Procurement and Contract Management\",\n",
    "    \"Bachelor of Project Management\",\n",
    "    \"Bachelor of Business Administration\",\n",
    "    \"Bachelor of Journalism\",\n",
    "    \"Bachelor of Business and Office Management\",\n",
    "    \"Bachelor of Economics and Statistics\",\n",
    "    \"Bachelor of Mass Communication\",\n",
    "    \"Bachelor of Commerce\",\n",
    "    \"Bachelor of Procurement and Logistics\",\n",
    "    \"Bachelor of Finance\",\n",
    "    \"Bachelor of Business Information Technology\",\n",
    "    \"Bachelor of Technology and Entrepreneurship Management\",\n",
    "]\n",
    "\n",
    "def Recommendations(gender, extracurricular_activities,\n",
    "                    riasec, math_score, history_score, physics_score,\n",
    "                    chemistry_score, biology_score, english_score, geography_score,\n",
    "                    total_score,average_score):\n",
    "    \n",
    "    # Encode categorical variables\n",
    "    gender_encoded = 1 if gender.lower() == 'female' else 0\n",
    "    extracurricular_activities_encoded = 1 if extracurricular_activities else 0\n",
    "    \n",
    "    # Create feature array\n",
    "    feature_array = np.array([[gender_encoded, extracurricular_activities_encoded,\n",
    "                               riasec, math_score, history_score, physics_score,\n",
    "                               chemistry_score, biology_score, english_score, geography_score,total_score,average_score]])\n",
    "    \n",
    "    # Scale features\n",
    "    scaled_features = scaler.transform(feature_array)\n",
    "    \n",
    "    # Predict using the model\n",
    "    probabilities = model.predict_proba(scaled_features)\n",
    "    \n",
    "    # Get top five predicted classes along with their probabilities\n",
    "    top_classes_idx = np.argsort(-probabilities[0])[:5]\n",
    "    top_classes_names_probs = [(class_names[idx], probabilities[0][idx]) for idx in top_classes_idx]\n",
    "    \n",
    "    return top_classes_names_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c9d10c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top recommended studies with probabilities:\n",
      "==================================================\n",
      "Bachelor of Project Management with probability 0.54\n",
      "Bachelor of Development Studies with probability 0.2\n",
      "Bachelor of Business Administration with probability 0.09\n",
      "Bachelor of Business Information Technology with probability 0.04\n",
      "Bachelor of Finance with probability 0.03\n"
     ]
    }
   ],
   "source": [
    "# Example usage 1\n",
    "final_recommendations = Recommendations(gender='female',\n",
    "                                        extracurricular_activities=False,\n",
    "                                        riasec=32,\n",
    "                                        math_score=65,\n",
    "                                        history_score=60,\n",
    "                                        physics_score=97,\n",
    "                                        chemistry_score=94,\n",
    "                                        biology_score=71,\n",
    "                                        english_score=81,\n",
    "                                        geography_score=66,\n",
    "                                        total_score=534,\n",
    "                                        average_score=76.285714)\n",
    "\n",
    "print(\"Top recommended studies with probabilities:\")\n",
    "print(\"=\"*50)\n",
    "for class_name, probability in final_recommendations:\n",
    "    print(f\"{class_name} with probability {probability}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "27160dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top recommended studies with probabilities:\n",
      "==================================================\n",
      "Bachelor of Human Resouce Management with probability 0.44\n",
      "Bachelor of Business Administration with probability 0.2\n",
      "Bachelor of Finance with probability 0.1\n",
      "Bachelor of Corporate Communications with probability 0.06\n",
      "Bachelor of Project Management with probability 0.06\n"
     ]
    }
   ],
   "source": [
    "# Example usage 2\n",
    "final_recommendations = Recommendations(gender='female',\n",
    "                                        extracurricular_activities=False,\n",
    "                                        riasec=40,\n",
    "                                        math_score=87,\n",
    "                                        history_score=73,\n",
    "                                        physics_score=67,\n",
    "                                        chemistry_score=91,\n",
    "                                        biology_score=79,\n",
    "                                        english_score=60,\n",
    "                                        geography_score=77,\n",
    "                                        total_score=583,\n",
    "                                        average_score=83.285714)\n",
    "\n",
    "print(\"Top recommended studies with probabilities:\")\n",
    "print(\"=\"*50)\n",
    "for class_name, probability in final_recommendations:\n",
    "    print(f\"{class_name} with probability {probability}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3c9aca69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.1.post1\n"
     ]
    }
   ],
   "source": [
    "# sklear version in pychar production \n",
    "import sklearn\n",
    "print(sklearn.__version__)\n",
    "# in pycharm env install\n",
    "# pip install scikit-learn==1.3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b058bf0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
